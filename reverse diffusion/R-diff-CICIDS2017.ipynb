{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d70acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\thari\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\thari\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "Epoch 1/50, Loss: 73679806464.0\n",
      "Epoch 2/50, Loss: 32025475072.0\n",
      "Epoch 3/50, Loss: 17187530752.0\n",
      "Epoch 4/50, Loss: 9119132672.0\n",
      "Epoch 5/50, Loss: 5388014080.0\n",
      "Epoch 6/50, Loss: 4026478848.0\n",
      "Epoch 7/50, Loss: 3655920384.0\n",
      "Epoch 8/50, Loss: 3081977344.0\n",
      "Epoch 9/50, Loss: 2680680192.0\n",
      "Epoch 10/50, Loss: 2824721664.0\n",
      "Epoch 11/50, Loss: 4238984192.0\n",
      "Epoch 12/50, Loss: 3036802816.0\n",
      "Epoch 13/50, Loss: 2288489984.0\n",
      "Epoch 14/50, Loss: 2044369536.0\n",
      "Epoch 15/50, Loss: 2845809408.0\n",
      "Epoch 16/50, Loss: 2145673472.0\n",
      "Epoch 17/50, Loss: 1774596608.0\n",
      "Epoch 18/50, Loss: 1700304000.0\n",
      "Epoch 19/50, Loss: 4041637376.0\n",
      "Epoch 20/50, Loss: 3025621760.0\n",
      "Epoch 21/50, Loss: 2300027136.0\n",
      "Epoch 22/50, Loss: 3869168896.0\n",
      "Epoch 23/50, Loss: 2644785664.0\n",
      "Epoch 24/50, Loss: 1414704384.0\n",
      "Epoch 25/50, Loss: 1870813440.0\n",
      "Epoch 26/50, Loss: 1332093184.0\n",
      "Epoch 27/50, Loss: 2069321600.0\n",
      "Epoch 28/50, Loss: 1748551680.0\n",
      "Epoch 29/50, Loss: 1713818240.0\n",
      "Epoch 30/50, Loss: 1315737600.0\n",
      "Epoch 31/50, Loss: 1318360832.0\n",
      "Epoch 32/50, Loss: 2158255872.0\n",
      "Epoch 33/50, Loss: 1142866304.0\n",
      "Epoch 34/50, Loss: 1373475072.0\n",
      "Epoch 35/50, Loss: 1220067072.0\n",
      "Epoch 36/50, Loss: 1226582144.0\n",
      "Epoch 37/50, Loss: 2008046208.0\n",
      "Epoch 38/50, Loss: 1209350656.0\n",
      "Epoch 39/50, Loss: 1212706432.0\n",
      "Epoch 40/50, Loss: 1528136832.0\n",
      "Epoch 41/50, Loss: 1508263424.0\n",
      "Epoch 42/50, Loss: 1919652480.0\n",
      "Epoch 43/50, Loss: 1243455744.0\n",
      "Epoch 44/50, Loss: 1385257600.0\n",
      "Epoch 45/50, Loss: 1717962624.0\n",
      "Epoch 46/50, Loss: 2066655616.0\n",
      "Epoch 47/50, Loss: 2052388992.0\n",
      "Epoch 48/50, Loss: 978223680.0\n",
      "Epoch 49/50, Loss: 1015595968.0\n",
      "Epoch 50/50, Loss: 1485260160.0\n",
      "OOD Samples Shape: (10000, 30)\n",
      "OOD Labels Shape: (10000, 1)\n",
      "Sample OOD Labels: [[0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Load the data\n",
    "data_clean = pd.read_csv('cleansample_cicids2017.csv')\n",
    "\n",
    "# Sample 45000 rows from the data\n",
    "df = data_clean.sample(n=45000, random_state=42)\n",
    "\n",
    "# Separate features and labels from the DataFrame\n",
    "features = df.drop(columns=[\"Label\"]).values  # 30 features\n",
    "labels = df[\"Label\"].values  # Label column (last column)\n",
    "\n",
    "# No scaling is applied, keeping the features in their original scale\n",
    "features_scaled = features  \n",
    "\n",
    "# Parameters for the diffusion model\n",
    "timesteps = 1000\n",
    "embedding_dim = 128  # Adjust based on your data\n",
    "input_dim = features_scaled.shape[1] \n",
    "num_classes = len(np.unique(labels))\n",
    "\n",
    "# Noise schedule (betas for the diffusion process)\n",
    "def get_noise_schedule(timesteps):\n",
    "    beta_start = 0.0001\n",
    "    beta_end = 0.02\n",
    "    return np.linspace(beta_start, beta_end, timesteps)\n",
    "\n",
    "betas = get_noise_schedule(timesteps)\n",
    "\n",
    "# Forward noise process (adding noise to data)\n",
    "def forward_noise(x, t):\n",
    "    noise = np.random.normal(size=x.shape)\n",
    "    return np.sqrt(1 - betas[t]) * x + np.sqrt(betas[t]) * noise\n",
    "\n",
    "# Build conditional reverse model (MLP-based), with labels\n",
    "def build_conditional_reverse_model(input_dim, embedding_dim, num_classes):\n",
    "    input_data = layers.Input(shape=(input_dim,))\n",
    "    input_label = layers.Input(shape=(num_classes,))  # Labels one-hot encoded\n",
    "\n",
    "    # Concatenate data and label\n",
    "    x = layers.concatenate([input_data, input_label])\n",
    "    x = layers.Dense(embedding_dim, activation='relu')(x)\n",
    "    x = layers.Dense(embedding_dim, activation='relu')(x)\n",
    "    output = layers.Dense(input_dim)(x)  # Output is the denoised (or OOD) data\n",
    "\n",
    "    model = models.Model(inputs=[input_data, input_label], outputs=output)\n",
    "    return model\n",
    "\n",
    "# Loss function for reverse diffusion\n",
    "def diffusion_loss(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "\n",
    "# Training the reverse diffusion model with label conditioning\n",
    "def train_conditional_reverse_diffusion_model(model, data, labels, timesteps, epochs=50, batch_size=32):\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)  # Lower learning rate\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for step in range(0, len(data), batch_size):\n",
    "            x_batch = data[step:step+batch_size]\n",
    "            y_batch = labels[step:step+batch_size]\n",
    "            \n",
    "            t = np.random.randint(0, timesteps)  # Randomly choose a timestep\n",
    "            noisy_data = forward_noise(x_batch, t)  # Add noise to data\n",
    "\n",
    "            # Train model to predict the clean data from noisy data\n",
    "            with tf.GradientTape() as tape:\n",
    "                predictions = model([noisy_data, y_batch], training=True)\n",
    "                loss = diffusion_loss(x_batch, predictions)\n",
    "            \n",
    "            gradients = tape.gradient(loss, model.trainable_weights)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.numpy()}\")\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "labels_one_hot = tf.keras.utils.to_categorical(labels, num_classes=num_classes)\n",
    "\n",
    "# Build the conditional reverse model\n",
    "reverse_model = build_conditional_reverse_model(input_dim, embedding_dim, num_classes)\n",
    "\n",
    "# Train reverse model with labeled data\n",
    "train_conditional_reverse_diffusion_model(reverse_model, features_scaled, labels_one_hot, timesteps, epochs=50)\n",
    "\n",
    "# Generate OOD samples without scaling, with labels in (10000, 1) format\n",
    "def generate_conditional_ood_samples_with_labels(model, num_samples, input_dim, num_classes, timesteps):\n",
    "    # Randomly sample classes as integers\n",
    "    random_classes = np.random.randint(0, num_classes, num_samples)  # Labels as integers, not one-hot\n",
    "    \n",
    "    # Start with random noise\n",
    "    noise = np.random.normal(size=(num_samples, input_dim))  # Keep the noise in the original scale\n",
    "    \n",
    "    # Iteratively apply reverse diffusion\n",
    "    for t in reversed(range(timesteps)):\n",
    "        noise = model([noise, tf.keras.utils.to_categorical(random_classes, num_classes=num_classes)], training=False)  # Predict the clean data\n",
    "        # Optionally, can add noise back to keep it stochastic\n",
    "        noise = np.sqrt(1 - betas[t]) * noise + np.sqrt(betas[t]) * np.random.normal(size=noise.shape)\n",
    "    \n",
    "    # Return OOD samples and labels in shape (10000, 1)\n",
    "    return noise, random_classes.reshape(-1, 1)  # Labels as (10000, 1) with values 0 or 1\n",
    "\n",
    "# Generate OOD samples with labels in (10000, 1) format\n",
    "ood_samples, ood_labels = generate_conditional_ood_samples_with_labels(reverse_model, num_samples=10000, input_dim=input_dim, num_classes=num_classes, timesteps=timesteps)\n",
    "\n",
    "# Check the output shape\n",
    "print(\"OOD Samples Shape:\", ood_samples.shape)  # Should be (10000, input_dim)\n",
    "print(\"OOD Labels Shape:\", ood_labels.shape)    # Should be (10000, 1)\n",
    "print(\"Sample OOD Labels:\", ood_labels[:10])\n",
    "\n",
    "# Create a Pandas DataFrame for OOD samples and labels\n",
    "ood_samples_np = ood_samples.numpy() if isinstance(ood_samples, tf.Tensor) else ood_samples\n",
    "ood_labels_np = ood_labels.numpy() if isinstance(ood_labels, tf.Tensor) else ood_labels\n",
    "\n",
    "# Define column names for OOD samples based on your original dataset\n",
    "column_names = df.drop(columns=[\"Label\"]).columns.tolist()\n",
    "\n",
    "# Create a Pandas DataFrame for the OOD samples\n",
    "ood_samples_df = pd.DataFrame(ood_samples_np, columns=column_names)\n",
    "ood_labels_df = pd.DataFrame(ood_labels_np, columns=['Label'])\n",
    "\n",
    "# Concatenate the OOD samples and labels\n",
    "ood_combined_df = pd.concat([ood_samples_df, ood_labels_df], axis=1)\n",
    "\n",
    "# Save the OOD samples to CSV\n",
    "ood_combined_df.to_csv('ood_samples_CICIDS2017.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c49246b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf=pd.read_csv('ood_samples_CICIDS2017.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d009788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bwd Packet Length Std</th>\n",
       "      <th>Packet Length Std</th>\n",
       "      <th>Bwd Packet Length Max</th>\n",
       "      <th>Avg Bwd Segment Size</th>\n",
       "      <th>Bwd Packet Length Mean</th>\n",
       "      <th>Average Packet Size</th>\n",
       "      <th>Packet Length Variance</th>\n",
       "      <th>ACK Flag Count</th>\n",
       "      <th>Fwd IAT Std</th>\n",
       "      <th>Packet Length Mean</th>\n",
       "      <th>...</th>\n",
       "      <th>Idle Max</th>\n",
       "      <th>Flow Duration</th>\n",
       "      <th>Bwd IAT Total</th>\n",
       "      <th>act_data_pkt_fwd</th>\n",
       "      <th>Min Packet Length</th>\n",
       "      <th>Fwd IAT Mean</th>\n",
       "      <th>Bwd IAT Max</th>\n",
       "      <th>URG Flag Count</th>\n",
       "      <th>Fwd Packet Length Std</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.094918e+29</td>\n",
       "      <td>5.350024e+29</td>\n",
       "      <td>-1.962738e+30</td>\n",
       "      <td>3.810242e+29</td>\n",
       "      <td>-1.877740e+29</td>\n",
       "      <td>2.089048e+30</td>\n",
       "      <td>2.831911e+31</td>\n",
       "      <td>5.854916e+29</td>\n",
       "      <td>3.209356e+31</td>\n",
       "      <td>1.187300e+30</td>\n",
       "      <td>...</td>\n",
       "      <td>1.299891e+31</td>\n",
       "      <td>3.624817e+31</td>\n",
       "      <td>2.025851e+31</td>\n",
       "      <td>1.288902e+30</td>\n",
       "      <td>5.219913e+29</td>\n",
       "      <td>2.520785e+31</td>\n",
       "      <td>3.207500e+31</td>\n",
       "      <td>1.040691e+30</td>\n",
       "      <td>-3.588300e+29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-8.381894e+27</td>\n",
       "      <td>4.095300e+28</td>\n",
       "      <td>-1.502367e+29</td>\n",
       "      <td>2.916673e+28</td>\n",
       "      <td>-1.437232e+28</td>\n",
       "      <td>1.599047e+29</td>\n",
       "      <td>2.167653e+30</td>\n",
       "      <td>4.481571e+28</td>\n",
       "      <td>2.456566e+30</td>\n",
       "      <td>9.087922e+28</td>\n",
       "      <td>...</td>\n",
       "      <td>9.949805e+29</td>\n",
       "      <td>2.774563e+30</td>\n",
       "      <td>1.550658e+30</td>\n",
       "      <td>9.865701e+28</td>\n",
       "      <td>3.995619e+28</td>\n",
       "      <td>1.929511e+30</td>\n",
       "      <td>2.455145e+30</td>\n",
       "      <td>7.966081e+28</td>\n",
       "      <td>-2.746534e+28</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2.134693e+28</td>\n",
       "      <td>1.042887e+29</td>\n",
       "      <td>-3.825880e+29</td>\n",
       "      <td>7.427595e+28</td>\n",
       "      <td>-3.659997e+28</td>\n",
       "      <td>4.072041e+29</td>\n",
       "      <td>5.520051e+30</td>\n",
       "      <td>1.141248e+29</td>\n",
       "      <td>6.255781e+30</td>\n",
       "      <td>2.314299e+29</td>\n",
       "      <td>...</td>\n",
       "      <td>2.533771e+30</td>\n",
       "      <td>7.065574e+30</td>\n",
       "      <td>3.948836e+30</td>\n",
       "      <td>2.512353e+29</td>\n",
       "      <td>1.017510e+29</td>\n",
       "      <td>4.913614e+30</td>\n",
       "      <td>6.252162e+30</td>\n",
       "      <td>2.028612e+29</td>\n",
       "      <td>-6.994174e+28</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-2.310333e+28</td>\n",
       "      <td>1.128751e+29</td>\n",
       "      <td>-4.140927e+29</td>\n",
       "      <td>8.038998e+28</td>\n",
       "      <td>-3.961325e+28</td>\n",
       "      <td>4.407414e+29</td>\n",
       "      <td>5.974637e+30</td>\n",
       "      <td>1.235250e+29</td>\n",
       "      <td>6.770955e+30</td>\n",
       "      <td>2.504889e+29</td>\n",
       "      <td>...</td>\n",
       "      <td>2.742434e+30</td>\n",
       "      <td>7.647444e+30</td>\n",
       "      <td>4.274031e+30</td>\n",
       "      <td>2.719265e+29</td>\n",
       "      <td>1.101292e+29</td>\n",
       "      <td>5.318248e+30</td>\n",
       "      <td>6.767040e+30</td>\n",
       "      <td>2.195660e+29</td>\n",
       "      <td>-7.570301e+28</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-2.590786e+28</td>\n",
       "      <td>1.265932e+29</td>\n",
       "      <td>-4.644165e+29</td>\n",
       "      <td>9.016211e+28</td>\n",
       "      <td>-4.442834e+28</td>\n",
       "      <td>4.942980e+29</td>\n",
       "      <td>6.700713e+30</td>\n",
       "      <td>1.385351e+29</td>\n",
       "      <td>7.593800e+30</td>\n",
       "      <td>2.809302e+29</td>\n",
       "      <td>...</td>\n",
       "      <td>3.075716e+30</td>\n",
       "      <td>8.576815e+30</td>\n",
       "      <td>4.793439e+30</td>\n",
       "      <td>3.049723e+29</td>\n",
       "      <td>1.235155e+29</td>\n",
       "      <td>5.964552e+30</td>\n",
       "      <td>7.589411e+30</td>\n",
       "      <td>2.462472e+29</td>\n",
       "      <td>-8.490107e+28</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Bwd Packet Length Std   Packet Length Std  Bwd Packet Length Max  \\\n",
       "0           -1.094918e+29        5.350024e+29          -1.962738e+30   \n",
       "1           -8.381894e+27        4.095300e+28          -1.502367e+29   \n",
       "2           -2.134693e+28        1.042887e+29          -3.825880e+29   \n",
       "3           -2.310333e+28        1.128751e+29          -4.140927e+29   \n",
       "4           -2.590786e+28        1.265932e+29          -4.644165e+29   \n",
       "\n",
       "    Avg Bwd Segment Size   Bwd Packet Length Mean   Average Packet Size  \\\n",
       "0           3.810242e+29            -1.877740e+29          2.089048e+30   \n",
       "1           2.916673e+28            -1.437232e+28          1.599047e+29   \n",
       "2           7.427595e+28            -3.659997e+28          4.072041e+29   \n",
       "3           8.038998e+28            -3.961325e+28          4.407414e+29   \n",
       "4           9.016211e+28            -4.442834e+28          4.942980e+29   \n",
       "\n",
       "    Packet Length Variance   ACK Flag Count   Fwd IAT Std  \\\n",
       "0             2.831911e+31     5.854916e+29  3.209356e+31   \n",
       "1             2.167653e+30     4.481571e+28  2.456566e+30   \n",
       "2             5.520051e+30     1.141248e+29  6.255781e+30   \n",
       "3             5.974637e+30     1.235250e+29  6.770955e+30   \n",
       "4             6.700713e+30     1.385351e+29  7.593800e+30   \n",
       "\n",
       "    Packet Length Mean  ...      Idle Max   Flow Duration  Bwd IAT Total  \\\n",
       "0         1.187300e+30  ...  1.299891e+31    3.624817e+31   2.025851e+31   \n",
       "1         9.087922e+28  ...  9.949805e+29    2.774563e+30   1.550658e+30   \n",
       "2         2.314299e+29  ...  2.533771e+30    7.065574e+30   3.948836e+30   \n",
       "3         2.504889e+29  ...  2.742434e+30    7.647444e+30   4.274031e+30   \n",
       "4         2.809302e+29  ...  3.075716e+30    8.576815e+30   4.793439e+30   \n",
       "\n",
       "    act_data_pkt_fwd   Min Packet Length   Fwd IAT Mean   Bwd IAT Max  \\\n",
       "0       1.288902e+30        5.219913e+29   2.520785e+31  3.207500e+31   \n",
       "1       9.865701e+28        3.995619e+28   1.929511e+30  2.455145e+30   \n",
       "2       2.512353e+29        1.017510e+29   4.913614e+30  6.252162e+30   \n",
       "3       2.719265e+29        1.101292e+29   5.318248e+30  6.767040e+30   \n",
       "4       3.049723e+29        1.235155e+29   5.964552e+30  7.589411e+30   \n",
       "\n",
       "    URG Flag Count   Fwd Packet Length Std  Label  \n",
       "0     1.040691e+30           -3.588300e+29      0  \n",
       "1     7.966081e+28           -2.746534e+28      1  \n",
       "2     2.028612e+29           -6.994174e+28      1  \n",
       "3     2.195660e+29           -7.570301e+28      1  \n",
       "4     2.462472e+29           -8.490107e+28      1  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ed6fe4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
