{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565d134d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "import pickle # saving and loading trained model\n",
    "from os import path\n",
    "\n",
    "# importing required libraries for normalizing data\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# importing library for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score # for calculating accuracy of model\n",
    "from sklearn.model_selection import train_test_split # for splitting the dataset for training and testing\n",
    "from sklearn.metrics import classification_report # for generating a classification report of model\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import tensorflow  as keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, Activation\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b9cb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data_clean = pd.read_csv('cleansample_cicids2017.csv')# use each data set one by one\n",
    "\n",
    "# # Load the data\n",
    "# data_clean = pd.read_csv('cleansample_ciciot23.csv')\n",
    "\n",
    "# # Load the data\n",
    "# data_clean = pd.read_csv('cleansample_insdn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de62221",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_clean['Label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648d55b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = data_clean['Label'].values\n",
    "data = data_clean.drop(columns=['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3fda9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "_features = X_train.shape[1]\n",
    "# n_classes = labels.shape[1]\n",
    "\n",
    "print('X.shape = ',X_train.shape)\n",
    "print('Y.shape = ',labels.shape)\n",
    "print('X_train.shape = ',X_train.shape)\n",
    "# print('y_train.shape = ', Y_train.shape)\n",
    "print('X_test.shape = ', X_test.shape)\n",
    "# print('y_test.shape = ',Y_test.shape)\n",
    "\n",
    "# print('X_val.shape = ', X_val.shape)\n",
    "# print('y_val.shape = ',Y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48f4466",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Define the VAE model\n",
    "input_dim = X_train.shape[1]\n",
    "latent_dim = 128 # Number of latent dimensions\n",
    "\n",
    "# Encoder\n",
    "inputs = layers.Input(shape=(input_dim,))\n",
    "h = layers.Dense(128, activation='relu')(inputs)  # Additional layer\n",
    "h = layers.Dense(64, activation='relu')(h) \n",
    "h = layers.Dense(32, activation='relu')(h)        # Additional layer\n",
    "h = layers.Dense(16, activation='relu')(h)\n",
    "z_mean = layers.Dense(latent_dim)(h)\n",
    "z_log_var = layers.Dense(latent_dim)(h)\n",
    "\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "z = layers.Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
    "\n",
    "# Decoder\n",
    "decoder_h = layers.Dense(16, activation='relu')(z)\n",
    "decoder_h = layers.Dense(32, activation='relu')(decoder_h)\n",
    "decoder_h = layers.Dense(64, activation='relu')(decoder_h)  # Additional layer\n",
    "decoder_h = layers.Dense(128, activation='relu')(decoder_h)  # Additional layer\n",
    "decoder_mean = layers.Dense(input_dim, activation='relu')\n",
    "h_decoded = decoder_h\n",
    "x_decoded_mean = decoder_mean(h_decoded)\n",
    "\n",
    "# VAE Model\n",
    "vae = Model(inputs, x_decoded_mean)\n",
    "\n",
    "# Loss Function\n",
    "reconstruction_loss = tf.keras.losses.mean_squared_error(inputs, x_decoded_mean)\n",
    "reconstruction_loss *= input_dim\n",
    "kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
    "kl_loss = K.sum(kl_loss, axis=-1)\n",
    "kl_loss *= -0.5\n",
    "vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
    "vae.add_loss(vae_loss)\n",
    "\n",
    "vae.compile(optimizer='adam')\n",
    "vae.summary()\n",
    "\n",
    "\n",
    "# Set up ModelCheckpoint to save the best model based on validation loss\n",
    "checkpoint = ModelCheckpoint('best_VAE_model.h5', \n",
    "                             monitor='val_loss', \n",
    "                             save_best_only=True, \n",
    "                             mode='min', \n",
    "                             verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ab02b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train the VAE\n",
    "# history = vae.fit(X_train[np.where(y_train==1)], X_train[np.where(y_train==1)], \n",
    "#                   shuffle=True, epochs=50, batch_size=32, validation_split=0.2)\n",
    "import time\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Train the VAE\n",
    "history = vae.fit(X_train[np.where(y_train==1)], X_train[np.where(y_train==1)], \n",
    "                  shuffle=True, epochs=50, batch_size=32, validation_split=0.2, callbacks=[checkpoint])\n",
    "\n",
    "\n",
    "vae.load_weights('best_VAE_model.h5')\n",
    "# End the timer\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate the training time\n",
    "training_time = end_time - start_time\n",
    "\n",
    "print(f\"Test time: {training_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7de5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01f9276",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db472fe2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ef9136",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b77c83a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
