{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371763e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# from unidecode import unidecode\n",
    "\n",
    "import pickle # saving and loading trained model\n",
    "from os import path\n",
    "\n",
    "# importing required libraries for normalizing data\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# importing library for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score # for calculating accuracy of model\n",
    "from sklearn.model_selection import train_test_split # for splitting the dataset for training and testing\n",
    "from sklearn.metrics import classification_report # for generating a classification report of model\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import tensorflow  as keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, Activation\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f22628f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data_clean = pd.read_csv('cleansample_cicids2017.csv')# use each data set one by one\n",
    "\n",
    "# # Load the data\n",
    "# data_clean = pd.read_csv('cleansample_ciciot23.csv')\n",
    "\n",
    "# # Load the data\n",
    "# data_clean = pd.read_csv('cleansample_insdn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38c7a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_clean['Label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546c7384",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = data_clean['Label'].values\n",
    "data = data_clean.drop(columns=['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87468ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "_features = X_train.shape[1]\n",
    "# n_classes = labels.shape[1]\n",
    "\n",
    "print('X.shape = ',X_train.shape)\n",
    "print('Y.shape = ',labels.shape)\n",
    "print('X_train.shape = ',X_train.shape)\n",
    "# print('y_train.shape = ', Y_train.shape)\n",
    "print('X_test.shape = ', X_test.shape)\n",
    "# print('y_test.shape = ',Y_test.shape)\n",
    "\n",
    "# print('X_val.shape = ', X_val.shape)\n",
    "# print('y_val.shape = ',Y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7df3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Define the model architecture\n",
    "def build_dae(input_dim):\n",
    "    # Encoder\n",
    "    input_layer = layers.Input(shape=(input_dim,))  \n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dropout(0.5)(x)  # Dropout layer for regularization\n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "    x = layers.Dense(32, activation='relu')(x)\n",
    "    x = layers.Dense(16, activation='relu')(x)\n",
    "    # Bottleneck\n",
    "    encoded = layers.Dense(4, activation='relu')(x)  # Latent space\n",
    "    \n",
    "    # Decoder\n",
    "    x = layers.Dense(16, activation='relu')(encoded)\n",
    "    x = layers.Dense(32, activation='relu')(x)\n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    decoded = layers.Dense(input_dim, activation='relu')(x)\n",
    "    \n",
    "    # Full Autoencoder Model\n",
    "    autoencoder = models.Model(inputs=input_layer, outputs=decoded)\n",
    "    \n",
    "    # Compile the model\n",
    "    autoencoder.compile(optimizer='adam', loss='mse')\n",
    "    \n",
    "    # Encoder Model (up to the latent space)\n",
    "    encoder = models.Model(inputs=input_layer, outputs=encoded)\n",
    "    \n",
    "    return autoencoder, encoder\n",
    "\n",
    "\n",
    "input_dim = X_train.shape[1]  # Example feature dimension, update according to your data\n",
    "autoencoder, encoder = build_dae(input_dim)\n",
    "autoencoder.summary()\n",
    "#final\n",
    "# Set up ModelCheckpoint to save the best model based on validation loss\n",
    "checkpoint = ModelCheckpoint('best_DAE_model.h5', \n",
    "                             monitor='val_loss', \n",
    "                             save_best_only=True, \n",
    "                             mode='min', \n",
    "                             verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700f5bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_factor = 0.1\n",
    "x_train_noisy = X_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=X_train.shape)\n",
    "x_test_noisy = X_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fdf1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "history=autoencoder.fit(x_train_noisy[np.where(y_train==1)],X_train[np.where(y_train==1)],\n",
    "               epochs=50,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test_noisy, X_test),\n",
    "                validation_split=0.2\n",
    "                       )\n",
    "\n",
    "# After training, load the best weights\n",
    "# autoencoder.load_weights('best_DAE_model.h5')\n",
    "\n",
    "# End the timer\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate the training time\n",
    "training_time = end_time - start_time\n",
    "\n",
    "print(f\"Test time: {training_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9217703a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylim(0,0.09)\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e6332c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score, precision_recall_curve, auc, f1_score, f1_score\n",
    "\n",
    "import time\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "# Use the autoencoder to make predictions\n",
    "predictions = autoencoder.predict(X_test)\n",
    "\n",
    "# Calculate the reconstruction error\n",
    "mse = np.mean(np.power(X_test - predictions, 2), axis=1)\n",
    "\n",
    "# Define a range of percentiles to test as thresholds\n",
    "percentiles = np.arange(1, 100, 1)  # Testing percentiles from 1% to 99%\n",
    "\n",
    "# Initialize variables to store the best metrics and threshold\n",
    "best_threshold = 0\n",
    "best_macro_f1 = 0\n",
    "best_accuracy = 0\n",
    "best_conf_matrix = None\n",
    "best_class_report = None\n",
    "best_roc_auc = 0\n",
    "best_pr_auc = 0\n",
    "\n",
    "for p in percentiles:\n",
    "    # Calculate the threshold for the current percentile\n",
    "    threshold = np.percentile(mse, p)\n",
    "    \n",
    "    # Classify as anomaly if mse > threshold\n",
    "    predicted_labels = (mse > threshold).astype(int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, predicted_labels)\n",
    "    macro_f1 = f1_score(y_test, predicted_labels, average='macro')\n",
    "    conf_matrix = confusion_matrix(y_test, predicted_labels)\n",
    "    class_report = classification_report(y_test, predicted_labels)\n",
    "    roc_auc = roc_auc_score(y_test, mse)\n",
    "    precision, recall, _ = precision_recall_curve(y_test, mse)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    \n",
    "    # If the current macro F1-score is better than the best one, update the best metrics\n",
    "    if macro_f1 > best_macro_f1:\n",
    "        best_macro_f1 = macro_f1\n",
    "        best_threshold = threshold\n",
    "        best_accuracy = accuracy\n",
    "        best_conf_matrix = conf_matrix\n",
    "        best_class_report = class_report\n",
    "        best_roc_auc = roc_auc\n",
    "        best_pr_auc = pr_auc\n",
    "\n",
    "# Output the best results\n",
    "print(f'Optimal Percentile Threshold: {best_threshold} (percentile: {p}%)')\n",
    "print(f'Best Macro F1-Score: {best_macro_f1}')\n",
    "print(f'Accuracy at Best Macro F1-Score: {best_accuracy}')\n",
    "print('Confusion Matrix at Best Macro F1-Score:')\n",
    "print(best_conf_matrix)\n",
    "print('\\nClassification Report at Best Macro F1-Score:')\n",
    "print(best_class_report)\n",
    "print(f'ROC-AUC at Best Macro F1-Score: {best_roc_auc}')\n",
    "print(f'PR-AUC at Best Macro F1-Score: {best_pr_auc}')\n",
    "\n",
    "\n",
    "# End the timer\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate the training time\n",
    "training_time = end_time - start_time\n",
    "\n",
    "print(f\"Test time: {training_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7302da2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Precision-Recall curve\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score, precision_recall_curve, auc, PrecisionRecallDisplay\n",
    "PrecisionRecallDisplay(precision=precision, recall=recall).plot()\n",
    "plt.title('Precision-Recall curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460a46dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, RocCurveDisplay, roc_auc_score\n",
    "\n",
    "# Assuming mse is already calculated for the reconstruction error\n",
    "# and y_test contains the true labels\n",
    "\n",
    "# Compute ROC curve and ROC area\n",
    "fpr, tpr, _ = roc_curve(y_test, mse)\n",
    "roc_auc = roc_auc_score(y_test, mse)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name='Autoencoder').plot()\n",
    "plt.title(f'Receiver Operating Characteristic (ROC) Curve\\nROC-AUC: {roc_auc:.2f}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4783b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input\n",
    "# Latent Space Visualization with PCA\n",
    "# encoder = models.Model(inputs=input_layer, outputs=decoded)\n",
    "encoded_X_test = encoder.predict(X_test)\n",
    "\n",
    "# Normalize the encoded data\n",
    "scaler = StandardScaler()\n",
    "encoded_X_test_normalized = scaler.fit_transform(encoded_X_test)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=2)\n",
    "encoded_X_test_pca = pca.fit_transform(encoded_X_test_normalized)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(encoded_X_test_pca[:, 0], encoded_X_test_pca[:, 1], c=y_test, cmap='viridis')\n",
    "plt.colorbar()\n",
    "plt.title('Latent Space Visualization using PCA')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd419297",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Encode the data into the latent space using the VAE encoder\n",
    "encoded_X_test = encoder.predict(X_test)\n",
    "\n",
    "# Step 2: Apply t-SNE to the latent representations\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "X_tsne = tsne.fit_transform(encoded_X_test)\n",
    "\n",
    "# Step 3: Plot the t-SNE results\n",
    "plt.figure(figsize=(8, 6))\n",
    "scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y_test, cmap='viridis', s=10)\n",
    "plt.colorbar(scatter)\n",
    "plt.title('t-SNE Visualization of VAE Latent Space')\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
