{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565d134d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "import pickle # saving and loading trained model\n",
    "from os import path\n",
    "\n",
    "# importing required libraries for normalizing data\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# importing library for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score # for calculating accuracy of model\n",
    "from sklearn.model_selection import train_test_split # for splitting the dataset for training and testing\n",
    "from sklearn.metrics import classification_report # for generating a classification report of model\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import tensorflow  as keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, Activation\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b9cb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data_clean = pd.read_csv('cleansample_cicids2017.csv')# use each data set one by one\n",
    "\n",
    "# # Load the data\n",
    "# data_clean = pd.read_csv('cleansample_ciciot23.csv')\n",
    "\n",
    "# # Load the data\n",
    "# data_clean = pd.read_csv('cleansample_insdn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de62221",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_clean['Label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648d55b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = data_clean['Label'].values\n",
    "data = data_clean.drop(columns=['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3fda9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "_features = X_train.shape[1]\n",
    "# n_classes = labels.shape[1]\n",
    "\n",
    "print('X.shape = ',X_train.shape)\n",
    "print('Y.shape = ',labels.shape)\n",
    "print('X_train.shape = ',X_train.shape)\n",
    "# print('y_train.shape = ', Y_train.shape)\n",
    "print('X_test.shape = ', X_test.shape)\n",
    "# print('y_test.shape = ',Y_test.shape)\n",
    "\n",
    "# print('X_val.shape = ', X_val.shape)\n",
    "# print('y_val.shape = ',Y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48f4466",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Define the VAE model\n",
    "input_dim = X_train.shape[1]\n",
    "latent_dim = 128 # Number of latent dimensions\n",
    "\n",
    "# Encoder\n",
    "inputs = layers.Input(shape=(input_dim,))\n",
    "h = layers.Dense(128, activation='relu')(inputs)  # Additional layer\n",
    "h = layers.Dense(64, activation='relu')(h) \n",
    "h = layers.Dense(32, activation='relu')(h)        # Additional layer\n",
    "h = layers.Dense(16, activation='relu')(h)\n",
    "z_mean = layers.Dense(latent_dim)(h)\n",
    "z_log_var = layers.Dense(latent_dim)(h)\n",
    "\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "z = layers.Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
    "\n",
    "# Decoder\n",
    "decoder_h = layers.Dense(16, activation='relu')(z)\n",
    "decoder_h = layers.Dense(32, activation='relu')(decoder_h)\n",
    "decoder_h = layers.Dense(64, activation='relu')(decoder_h)  # Additional layer\n",
    "decoder_h = layers.Dense(128, activation='relu')(decoder_h)  # Additional layer\n",
    "decoder_mean = layers.Dense(input_dim, activation='relu')\n",
    "h_decoded = decoder_h\n",
    "x_decoded_mean = decoder_mean(h_decoded)\n",
    "\n",
    "# VAE Model\n",
    "vae = Model(inputs, x_decoded_mean)\n",
    "\n",
    "# Loss Function\n",
    "reconstruction_loss = tf.keras.losses.mean_squared_error(inputs, x_decoded_mean)\n",
    "reconstruction_loss *= input_dim\n",
    "kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
    "kl_loss = K.sum(kl_loss, axis=-1)\n",
    "kl_loss *= -0.5\n",
    "vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
    "vae.add_loss(vae_loss)\n",
    "\n",
    "vae.compile(optimizer='adam')\n",
    "vae.summary()\n",
    "\n",
    "\n",
    "# Set up ModelCheckpoint to save the best model based on validation loss\n",
    "checkpoint = ModelCheckpoint('best_VAE_model.h5', \n",
    "                             monitor='val_loss', \n",
    "                             save_best_only=True, \n",
    "                             mode='min', \n",
    "                             verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ab02b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train the VAE\n",
    "# history = vae.fit(X_train[np.where(y_train==1)], X_train[np.where(y_train==1)], \n",
    "#                   shuffle=True, epochs=50, batch_size=32, validation_split=0.2)\n",
    "import time\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Train the VAE\n",
    "history = vae.fit(X_train[np.where(y_train==1)], X_train[np.where(y_train==1)], \n",
    "                  shuffle=True, epochs=50, batch_size=32, validation_split=0.2, callbacks=[checkpoint])\n",
    "\n",
    "\n",
    "vae.load_weights('best_VAE_model.h5')\n",
    "# End the timer\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate the training time\n",
    "training_time = end_time - start_time\n",
    "\n",
    "print(f\"Test time: {training_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7de5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01f9276",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score, precision_recall_curve, auc, f1_score, f1_score\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "# Use the autoencoder to make predictions\n",
    "predictions = vae.predict(X_test)\n",
    "\n",
    "# Calculate the reconstruction error\n",
    "mse = np.mean(np.power(X_test - predictions, 2), axis=1)\n",
    "\n",
    "# Define a range of percentiles to test as thresholds\n",
    "percentiles = np.arange(1, 100, 1)  # Testing percentiles from 1% to 99%\n",
    "\n",
    "# Initialize variables to store the best metrics and threshold\n",
    "best_threshold = 0\n",
    "best_macro_f1 = 0\n",
    "best_accuracy = 0\n",
    "best_conf_matrix = None\n",
    "best_class_report = None\n",
    "best_roc_auc = 0\n",
    "best_pr_auc = 0\n",
    "\n",
    "for p in percentiles:\n",
    "    # Calculate the threshold for the current percentile\n",
    "    threshold = np.percentile(mse, p)\n",
    "    \n",
    "    # Classify as anomaly if mse > threshold\n",
    "    predicted_labels = (mse > threshold).astype(int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, predicted_labels)\n",
    "    macro_f1 = f1_score(y_test, predicted_labels, average='macro')\n",
    "    conf_matrix = confusion_matrix(y_test, predicted_labels)\n",
    "    class_report = classification_report(y_test, predicted_labels)\n",
    "    roc_auc = roc_auc_score(y_test, mse)\n",
    "    precision, recall, _ = precision_recall_curve(y_test, mse)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    \n",
    "    # If the current macro F1-score is better than the best one, update the best metrics\n",
    "    if macro_f1 > best_macro_f1:\n",
    "        best_macro_f1 = macro_f1\n",
    "        best_threshold = threshold\n",
    "        best_accuracy = accuracy\n",
    "        best_conf_matrix = conf_matrix\n",
    "        best_class_report = class_report\n",
    "        best_roc_auc = roc_auc\n",
    "        best_pr_auc = pr_auc\n",
    "\n",
    "# Output the best results\n",
    "print(f'Optimal Percentile Threshold: {best_threshold} (percentile: {p}%)')\n",
    "print(f'Best Macro F1-Score: {best_macro_f1}')\n",
    "print(f'Accuracy at Best Macro F1-Score: {best_accuracy}')\n",
    "print('Confusion Matrix at Best Macro F1-Score:')\n",
    "print(best_conf_matrix)\n",
    "print('\\nClassification Report at Best Macro F1-Score:')\n",
    "print(best_class_report)\n",
    "print(f'ROC-AUC at Best Macro F1-Score: {best_roc_auc}')\n",
    "print(f'PR-AUC at Best Macro F1-Score: {best_pr_auc}')\n",
    "\n",
    "\n",
    "# End the timer\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate the training time\n",
    "training_time = end_time - start_time\n",
    "\n",
    "print(f\"Test time: {training_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db472fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score, precision_recall_curve, auc, PrecisionRecallDisplay\n",
    "# Plot Precision-Recall curve\n",
    "PrecisionRecallDisplay(precision=precision, recall=recall).plot()\n",
    "plt.title('Precision-Recall curve')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ef9136",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, RocCurveDisplay, roc_auc_score\n",
    "\n",
    "# Assuming mse is already calculated for the reconstruction error\n",
    "# and y_test contains the true labels\n",
    "\n",
    "# Compute ROC curve and ROC area\n",
    "fpr, tpr, _ = roc_curve(y_test, mse)\n",
    "roc_auc = roc_auc_score(y_test, mse)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name='Autoencoder').plot()\n",
    "plt.title(f'Receiver Operating Characteristic (ROC) Curve\\nROC-AUC: {roc_auc:.2f}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b77c83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score, precision_recall_curve, auc, f1_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from tensorflow.keras.layers import Input\n",
    "from sklearn.decomposition import PCA\n",
    "# Latent Space Visualization with PCA\n",
    "encoder = Model(inputs=inputs, outputs=x_decoded_mean)\n",
    "encoded_X_test = encoder.predict(X_test)\n",
    "\n",
    "# Normalize the encoded data\n",
    "scaler = StandardScaler()\n",
    "encoded_X_test_normalized = scaler.fit_transform(encoded_X_test)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=2)\n",
    "encoded_X_test_pca = pca.fit_transform(encoded_X_test_normalized)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(encoded_X_test_pca[:, 0], encoded_X_test_pca[:, 1], c=y_test, cmap='viridis')\n",
    "plt.colorbar()\n",
    "plt.title('Latent Space Visualization using PCA')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d144ea50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Encode the data into the latent space using the VAE encoder\n",
    "encoded_X_test = encoder.predict(X_test)\n",
    "\n",
    "# Step 2: Apply t-SNE to the latent representations\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "X_tsne = tsne.fit_transform(encoded_X_test)\n",
    "\n",
    "# Step 3: Plot the t-SNE results\n",
    "plt.figure(figsize=(8, 6))\n",
    "scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y_test, cmap='viridis', s=10)\n",
    "plt.colorbar(scatter)\n",
    "plt.title('t-SNE Visualization of VAE Latent Space')\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
