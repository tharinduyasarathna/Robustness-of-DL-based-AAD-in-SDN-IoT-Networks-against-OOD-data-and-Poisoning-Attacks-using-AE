{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d4b69e2",
   "metadata": {},
   "source": [
    "CTGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d8d0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ctgan import CTGAN\n",
    "from ctgan import load_demo\n",
    "from table_evaluator import load_data, TableEvaluator\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# Load the datasets one by one based on senario we need to test ood detection\n",
    "# here we use validation dataset from same dataset but previously model did not seen \n",
    "data1=pd.read_csv('val_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc45b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ood data set related to same data set we genarated via ctgan\n",
    "# \n",
    "synthetic_data_500e=pd.read_csv('ctgan_cicids2017_ood.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16fcdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2= pd.concat([data1,synthetic_data_500e])\n",
    "print(data2['Label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf910bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815222a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a0beb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels1 = data2['Label'].values\n",
    "data2 = data2.drop(columns=['Label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1441e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace infinities with NaNs\n",
    "data2.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Fill NaNs with column mean\n",
    "data2.fillna(data2.mean(), inplace=True)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "data2 = scaler.transform(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2e2833",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "# Load the saved model\n",
    "autoencoder = load_model('Best_AE_model.h5') # you need load the relevent model need test ood detection based on saved models from previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6280ef4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score, precision_recall_curve, auc, f1_score, f1_score\n",
    "\n",
    "import time\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "# Use the autoencoder to make predictions\n",
    "predictions = autoencoder.predict(data2)\n",
    "\n",
    "# Calculate the reconstruction error\n",
    "mse = np.mean(np.power(data2 - predictions, 2), axis=1)\n",
    "\n",
    "# Define a range of percentiles to test as thresholds\n",
    "percentiles = np.arange(1, 100, 1)  # Testing percentiles from 1% to 99%\n",
    "\n",
    "# Initialize variables to store the best metrics and threshold\n",
    "best_threshold = 0\n",
    "best_macro_f1 = 0\n",
    "best_accuracy = 0\n",
    "best_conf_matrix = None\n",
    "best_class_report = None\n",
    "best_roc_auc = 0\n",
    "best_pr_auc = 0\n",
    "\n",
    "for p in percentiles:\n",
    "    # Calculate the threshold for the current percentile\n",
    "    threshold = np.percentile(mse, p)\n",
    "    \n",
    "    # Classify as anomaly if mse > threshold\n",
    "    predicted_labels = (mse > threshold).astype(int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(labels1, predicted_labels)\n",
    "    macro_f1 = f1_score(labels1, predicted_labels, average='macro')\n",
    "    conf_matrix = confusion_matrix(labels1, predicted_labels)\n",
    "    class_report = classification_report(labels1, predicted_labels)\n",
    "    roc_auc = roc_auc_score(labels1, mse)\n",
    "    precision, recall, _ = precision_recall_curve(labels1, mse)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    \n",
    "    # If the current macro F1-score is better than the best one, update the best metrics\n",
    "    if macro_f1 > best_macro_f1:\n",
    "        best_macro_f1 = macro_f1\n",
    "        best_threshold = threshold\n",
    "        best_accuracy = accuracy\n",
    "        best_conf_matrix = conf_matrix\n",
    "        best_class_report = class_report\n",
    "        best_roc_auc = roc_auc\n",
    "        best_pr_auc = pr_auc\n",
    "\n",
    "# Output the best results\n",
    "print(f'Optimal Percentile Threshold: {best_threshold} (percentile: {p}%)')\n",
    "print(f'Best Macro F1-Score: {best_macro_f1}')\n",
    "print(f'Accuracy at Best Macro F1-Score: {best_accuracy}')\n",
    "print('Confusion Matrix at Best Macro F1-Score:')\n",
    "print(best_conf_matrix)\n",
    "print('\\nClassification Report at Best Macro F1-Score:')\n",
    "print(best_class_report)\n",
    "print(f'ROC-AUC at Best Macro F1-Score: {best_roc_auc}')\n",
    "print(f'PR-AUC at Best Macro F1-Score: {best_pr_auc}')\n",
    "\n",
    "\n",
    "# End the timer\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate the training time\n",
    "training_time = end_time - start_time\n",
    "\n",
    "print(f\"Test time: {training_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac17618d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, RocCurveDisplay, roc_auc_score\n",
    "\n",
    "# Assuming mse is already calculated for the reconstruction error\n",
    "# and y_test contains the true labels\n",
    "\n",
    "# Compute ROC curve and ROC area\n",
    "fpr, tpr, _ = roc_curve(labels1, mse)\n",
    "roc_auc = roc_auc_score(labels1, mse)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name='Autoencoder').plot()\n",
    "plt.title(f'Receiver Operating Characteristic (ROC) Curve\\nROC-AUC: {roc_auc:.2f}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc53141d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latent Space Visualization with PCA\n",
    "# encoder = Model(inputs=input_layer, outputs=encoded)\n",
    "encoded_data2 = encoder.predict(data2)\n",
    "\n",
    "# Normalize the encoded data\n",
    "scaler = StandardScaler()\n",
    "encoded_data2_normalized = scaler.fit_transform(encoded_data2)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=2)\n",
    "encoded_data2_pca = pca.fit_transform(encoded_data2_normalized)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(encoded_data2_pca[:, 0], encoded_data2_pca[:, 1], c=labels1, cmap='viridis')\n",
    "plt.colorbar()\n",
    "plt.title('Latent Space Visualization using PCA')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6f7ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Encode the data into the latent space using the VAE encoder\n",
    "encoded_X_test = encoder.predict(data2)\n",
    "\n",
    "# Step 2: Apply t-SNE to the latent representations\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "X_tsne = tsne.fit_transform(encoded_X_test)\n",
    "\n",
    "# Step 3: Plot the t-SNE results\n",
    "plt.figure(figsize=(8, 6))\n",
    "scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=labels1, cmap='viridis', s=10)\n",
    "plt.colorbar(scatter)\n",
    "plt.title('t-SNE Visualization of VAE Latent Space')\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194bacf1",
   "metadata": {},
   "source": [
    "Reverse diffusion based OOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9666e6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ood data set related to same data set we genarated via Reverse diffusion based model\n",
    "diff_ood=pd.read_csv('ood_samples_reversediff.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b505fb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets one by one based on senario we need to test ood detection\n",
    "# here we use validation dataset from same dataset but previously model did not seen \n",
    "data1=pd.read_csv('val_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ae4d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2= pd.concat([data1,diff_ood])\n",
    "\n",
    "print(data2['Label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6333bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels1 = data2['Label'].values\n",
    "data2 = data2.drop(columns=['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e645aa43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace infinities with NaNs\n",
    "data2.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Fill NaNs with column mean\n",
    "data2.fillna(data2.mean(), inplace=True)\n",
    "\n",
    "data2 = scaler.transform(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852f0cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score, precision_recall_curve, auc, f1_score, f1_score\n",
    "\n",
    "import time\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "# Use the autoencoder to make predictions\n",
    "predictions = autoencoder.predict(data2)\n",
    "\n",
    "# Calculate the reconstruction error\n",
    "mse = np.mean(np.power(data2 - predictions, 2), axis=1)\n",
    "\n",
    "# Define a range of percentiles to test as thresholds\n",
    "percentiles = np.arange(1, 100, 1)  # Testing percentiles from 1% to 99%\n",
    "\n",
    "# Initialize variables to store the best metrics and threshold\n",
    "best_threshold = 0\n",
    "best_macro_f1 = 0\n",
    "best_accuracy = 0\n",
    "best_conf_matrix = None\n",
    "best_class_report = None\n",
    "best_roc_auc = 0\n",
    "best_pr_auc = 0\n",
    "\n",
    "for p in percentiles:\n",
    "    # Calculate the threshold for the current percentile\n",
    "    threshold = np.percentile(mse, p)\n",
    "    \n",
    "    # Classify as anomaly if mse > threshold\n",
    "    predicted_labels = (mse > threshold).astype(int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(labels1, predicted_labels)\n",
    "    macro_f1 = f1_score(labels1, predicted_labels, average='macro')\n",
    "    conf_matrix = confusion_matrix(labels1, predicted_labels)\n",
    "    class_report = classification_report(labels1, predicted_labels)\n",
    "    roc_auc = roc_auc_score(labels1, mse)\n",
    "    precision, recall, _ = precision_recall_curve(labels1, mse)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    \n",
    "    # If the current macro F1-score is better than the best one, update the best metrics\n",
    "    if macro_f1 > best_macro_f1:\n",
    "        best_macro_f1 = macro_f1\n",
    "        best_threshold = threshold\n",
    "        best_accuracy = accuracy\n",
    "        best_conf_matrix = conf_matrix\n",
    "        best_class_report = class_report\n",
    "        best_roc_auc = roc_auc\n",
    "        best_pr_auc = pr_auc\n",
    "\n",
    "# Output the best results\n",
    "print(f'Optimal Percentile Threshold: {best_threshold} (percentile: {p}%)')\n",
    "print(f'Best Macro F1-Score: {best_macro_f1}')\n",
    "print(f'Accuracy at Best Macro F1-Score: {best_accuracy}')\n",
    "print('Confusion Matrix at Best Macro F1-Score:')\n",
    "print(best_conf_matrix)\n",
    "print('\\nClassification Report at Best Macro F1-Score:')\n",
    "print(best_class_report)\n",
    "print(f'ROC-AUC at Best Macro F1-Score: {best_roc_auc}')\n",
    "print(f'PR-AUC at Best Macro F1-Score: {best_pr_auc}')\n",
    "\n",
    "\n",
    "# End the timer\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate the training time\n",
    "training_time = end_time - start_time\n",
    "\n",
    "print(f\"Test time: {training_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2537ced5",
   "metadata": {},
   "source": [
    "Poison Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d023697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets one by one based on senario we need to test ood detection\n",
    "# here we use validation dataset from same dataset but previously model did not seen \n",
    "data1=pd.read_csv('val_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ec6ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "poison = pd.read_csv('poison.csv') # we genarate poison data using ART and you can find ART code and use it to genarate posion data related to same dataset you use to train the DL model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854cf98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2= pd.concat([data1,poison])\n",
    "\n",
    "print(data2['Label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b356f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels1 = data2['Label'].values\n",
    "data2 = data2.drop(columns=['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d919a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace infinities with NaNs\n",
    "data2.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Fill NaNs with column mean\n",
    "data2.fillna(data2.mean(), inplace=True)\n",
    "\n",
    "data2 = scaler.transform(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e5728d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score, precision_recall_curve, auc, f1_score\n",
    "\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "# Use the autoencoder to make predictions\n",
    "predictions = autoencoder.predict(data2)\n",
    "\n",
    "# Calculate the reconstruction error\n",
    "mse = np.mean(np.power(data2 - predictions, 2), axis=1)\n",
    "\n",
    "# Define a range of percentiles to test as thresholds\n",
    "percentiles = np.arange(1, 100, 0.5)  # Testing percentiles from 50% to 99%\n",
    "\n",
    "# Initialize variables to store the best metrics and threshold\n",
    "best_threshold = 0\n",
    "best_f1 = 0\n",
    "best_accuracy = 0\n",
    "best_conf_matrix = None\n",
    "best_class_report = None\n",
    "best_roc_auc = 0\n",
    "best_pr_auc = 0\n",
    "\n",
    "for p in percentiles:\n",
    "    # Calculate the threshold for the current percentile\n",
    "    threshold = np.percentile(mse, p)\n",
    "    \n",
    "    # Classify as anomaly if mse > threshold\n",
    "    predicted_labels = (mse > threshold).astype(int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(labels1, predicted_labels)\n",
    "    f1 = f1_score(labels1, predicted_labels)\n",
    "    conf_matrix = confusion_matrix(labels1, predicted_labels)\n",
    "    class_report = classification_report(labels1, predicted_labels)\n",
    "    roc_auc = roc_auc_score(labels1, mse)\n",
    "    precision, recall, _ = precision_recall_curve(labels1, mse)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    \n",
    "    # If the current F1-score is better than the best one, update the best metrics\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = threshold\n",
    "        best_accuracy = accuracy\n",
    "        best_conf_matrix = conf_matrix\n",
    "        best_class_report = class_report\n",
    "        best_roc_auc = roc_auc\n",
    "        best_pr_auc = pr_auc\n",
    "\n",
    "# Output the best results\n",
    "print(f'Optimal Percentile Threshold: {best_threshold} (percentile: {p}%)')\n",
    "print(f'Best F1-Score: {best_f1}')\n",
    "print(f'Accuracy at Best F1-Score: {best_accuracy}')\n",
    "print('Confusion Matrix at Best F1-Score:')\n",
    "print(best_conf_matrix)\n",
    "print('\\nClassification Report at Best F1-Score:')\n",
    "print(best_class_report)\n",
    "print(f'ROC-AUC at Best F1-Score: {best_roc_auc}')\n",
    "print(f'PR-AUC at Best F1-Score: {best_pr_auc}')\n",
    "\n",
    "\n",
    "# End the timer\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate the training time\n",
    "training_time = end_time - start_time\n",
    "\n",
    "print(f\"Test time: {training_time} seconds\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
